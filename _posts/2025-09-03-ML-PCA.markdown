---
layout: post
title: "Principal Component Analysis: Linear Algebra"
date: 2025-09-03 10:00:00 +0000
categories: Machine Learning
excerpt: "An exploration of Eigen Decomposition, Singular Value Decomposition, and Low Rank Matrix Factorization"
author: "Arifeen"
---

# Introduction

This post assumes that readers are already familiar with the concepts of **eigenvalues**, **eigenvectors**, **eigen decomposition**, **singular value decomposition (SVD)**, **matrix norms**, **four fundamental spaces (row, column, null and left null) of a matrix**,**correlation**, **mean**, and **variance**. Although these topics are briefly discussed here, having a solid understanding of them will help you follow the explanations more clearly.


# Eigenvectors and Eigenvalues

Letâ€™s say we have a vector $v \in \mathbb{R}^{3 \times 1}$. When we multiply this vector by a matrix $A \in \mathbb{R}^{3 \times 3}$, the resultannt vector $Av$ may change the **direction**, the **length (norm)**, or both.

- If $A$ is an **orthogonal matrix**, the length of $v$ is preserved, but the direction may change (like a rotation or reflection).  
- However, if there exists a nonzero vector $v$ such that $Av$ points in the **same direction** as $v$ (possibly scaled), then $v$ is called an **eigenvector** of $A$.

Formally, eigenvectors and eigenvalues satisfy:

$$
Av = \lambda v
$$

where:
- $v \neq 0$ is the **eigenvector**,  
- $\lambda \in \mathbb{R}$ is the corresponding **eigenvalue**.

#Eigen decomposition

Eigen decomposition is a way of breaking down a square matrix into its eigenvectors and eigenvalues.  
For a matrix $A \in \mathbb{R}^{n \times n}$, it can be written as:

$$
A = V \Lambda V^{-1}
$$
or,
$$
\Lambda = V A V^{-1}
$$

where $V$ is the matrix of eigenvectors and $\Lambda$ is the diagonal matrix of eigenvalues.

## Example
Now, lets see an example of a matrix and its **eigenvectors**. 

$$
A =
\begin{bmatrix}
1.5 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 1
\end{bmatrix}
\quad
V =
\begin{bmatrix}
-0.4320 & 0.7563 & -0.4913 \\
-0.5932 &  0.1720 & 0.7864 \\
-0.6793 &  -0.6312 & -0.3744
\end{bmatrix}
\quad
\Lambda =
\begin{bmatrix}
 0.1267 & 0 & 0 \\
0 & 1.2725  & 0 \\
0 & 0 & 3.1007
\end{bmatrix}
$$


This matrix is a **non-singular** matrix of size $3 \times 3$, and the **row space** and **column space** have the same dimension.  
We can visualize the **eigenvectors** in 3D as shown in the figure.

<p align="center">
  <img src="/images/eigviz.jpg" alt="Eigenvectors in 3D" width="400">
</p>

## nEW
From the figure, we can see the **eigenvectors** with their corresponding **eigenvalues**. An interesting observation is that this matrix is **symmetric**, and for **symmetric** matrices, the **eigenvectors** are always orthogonal to each other. This orthogonality property is a fundamental characteristic of **symmetric** matrices: when $A = A^T$, the eigenvectors corresponding to different eigenvalues form an orthogonal basis for the vector space. We can verify this by computing the dot products between any pair of eigenvectors, which will be approximately zero (within numerical precision). In contrast, if the matrix were not **symmetric**, we would generally not obtain orthogonal eigenvectors. Non-symmetric matrices can have eigenvectors that point in arbitrary directions relative to each other,
---

*This analysis demonstrates how mathematical concepts can be made more intuitive through computational tools and visualizations.*