---
layout: post
title: "Principal Component Analysis: Linear Algebra"
date: 2025-09-03 10:00:00 +0000
categories: Machine Learning
excerpt: "An exploration of Eigen Decomposition, Singular Value Decomposition, and Low Rank Matrix Factorization"
author: "Arifeen"
---

# Introduction

This post assumes that readers are already familiar with the concepts of **eigenvalues**, **eigenvectors**, **eigen decomposition**, **singular value decomposition (SVD)**, **matrix norms**, **four fundamental spaces (row, column, null and left null) of a matrix**,**correlation**, **mean**, and **variance**. Although these topics are briefly discussed here, having a solid understanding of them will help you follow the explanations more clearly.


# Eigenvectors and Eigenvalues

Letâ€™s say we have a vector $v \in \mathbb{R}^{3 \times 1}$. When we multiply this vector by a matrix $A \in \mathbb{R}^{3 \times 3}$, the resultannt vector $Av$ may change the **direction**, the **length (norm)**, or both.

- If $A$ is an **orthogonal matrix**, the length of $v$ is preserved, but the direction may change (like a rotation or reflection).  
- However, if there exists a nonzero vector $v$ such that $Av$ points in the **same direction** as $v$ (possibly scaled), then $v$ is called an **eigenvector** of $A$.

Formally, eigenvectors and eigenvalues satisfy:

$$
Av = \lambda v
$$

where:
- $v \neq 0$ is the **eigenvector**,  
- $\lambda \in \mathbb{R}$ is the corresponding **eigenvalue**.

#Eigen decomposition

Eigen decomposition is a way of breaking down a square matrix into its eigenvectors and eigenvalues.  
For a matrix $A \in \mathbb{R}^{n \times n}$, it can be written as:

$$
A = V \Lambda V^{-1}
$$
or,
$$
\Lambda = V A V^{-1}
$$

where $V$ is the matrix of eigenvectors and $\Lambda$ is the diagonal matrix of eigenvalues.

## Example
Now, lets see an example of a matrix and its **eigenvectors**. 

$$
A =
\begin{bmatrix}
1 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 1
\end{bmatrix}
\quad
V =
\begin{bmatrix}
-0.5774 & -0.7071 & 0.4082 \\
-0.5774 &  0.0000 & -0.8165 \\
-0.5774 &  0.7071 & 0.4082
\end{bmatrix}
\quad
\Lambda =
\begin{bmatrix}
0.0000 & 0 & 0 \\
0 & 1.0000 & 0 \\
0 & 0 & 3.0000
\end{bmatrix}
$$


This matrix is a **non-singular** matrix of size $3 \times 3$, and the **row space** and **column space** have the same dimension.  
We can visualize the **eigenvectors** in 3D as shown in the figure.

<p align="center">
  <img src="/images/eigenvectors_3d_plot.jpg" alt="Eigenvectors in 3D" width="400">
</p>

---

*This analysis demonstrates how mathematical concepts can be made more intuitive through computational tools and visualizations.*