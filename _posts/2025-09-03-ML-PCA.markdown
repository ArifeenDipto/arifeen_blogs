---
layout: post
title: "Principal Component Analysis: Linear Algebra"
date: 2025-09-03 10:00:00 +0000
categories: Machine Learning
excerpt: "An exploration of Eigen Decomposition, Singular Value Decomposition, and Low Rank Matrix Factorization"
author: "Arifeen"
---

# Introduction

This post assumes that the readers are familiar with Eigen value and Eigen vectors, Eigen decomposition, Singular value decomposition, Matrix norm, correlation, mean and variance. Although, I briefly discussed these concepts but having a full understanding may help you understand more clearly. 

# Eigenvectors and Eigenvalues

Letâ€™s say we have a vector $v \in \mathbb{R}^{3 \times 1}$.  
When we multiply this vector by a matrix $A \in \mathbb{R}^{3 \times 3}$, the result $Av$ may change the **direction**, the **length (norm)**, or both.

- If $A$ is an **orthogonal matrix**, the length of $v$ is preserved, but the direction may change (like a rotation or reflection).  
- However, if there exists a nonzero vector $v$ such that $Av$ points in the **same direction** as $v$ (possibly scaled), then $v$ is called an **eigenvector** of $A$.

Formally, eigenvectors and eigenvalues satisfy:

$$
Av = \lambda v
$$

where:
- $v \neq 0$ is the **eigenvector**,  
- $\lambda \in \mathbb{R}$ is the corresponding **eigenvalue**.


---

*This analysis demonstrates how mathematical concepts can be made more intuitive through computational tools and visualizations.*